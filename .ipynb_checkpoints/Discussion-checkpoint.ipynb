{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider the bigger questions.\n",
    "\n",
    "# Why bother (motivation)?\n",
    "A formal language for composing networks\n",
    "\n",
    "\n",
    "Aka weight tying at a bigger, more complex, more structured scale\n",
    "\n",
    "<img src=\"_Images/PoggioModnet.png\" alt=\"Modnet\">\n",
    "\n",
    "\n",
    "Viewing networks at a higher level. Where instead of considering the connections between neurons we are considering the connections between networks. (although it is really the same thing, as a neuron is just some function).\n",
    "\n",
    "<img src=\"_Images/PoggioModnetmat.png\" alt=\"Modnetmat\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we want it to work (ideal features)?\n",
    "Based on variables or functions? Aka edges or nodes?\n",
    "So the matrix entries are the edges/connections and the indexes are the variables.\n",
    "### Fatrix\n",
    "\n",
    "Let A,B,C,D be functions.\n",
    "\n",
    "Then E is a matrix of functions.\n",
    "$$E = \\begin{bmatrix} 0 & A & 0 \\\\ 0 & 0 & B \\\\ C & 0 & D \\end{bmatrix} \\\\$$\n",
    "\n",
    "Where the matrix encodes the relations between variables.\n",
    "\n",
    "### Higher levels\n",
    "\n",
    "I would then want to be able to say\n",
    "\n",
    "$$\n",
    "E = \\begin{bmatrix} A & B & B \\\\ 0 & 0 & C \\\\ D & D & 0 \\end{bmatrix} \\\\\n",
    "$$\n",
    "and even\n",
    "$$\n",
    "F = \\begin{bmatrix} E & 0 & 0 \\\\ A & E & B \\\\ C & 0 & D \\end{bmatrix} \\\\\n",
    "$$\n",
    "on so on and so forth, for as many higher 'levels' as I wanted. Making it easy to create 'meta' functions, and to see/code higher level structure.\n",
    "\n",
    "### Composition (algebra)\n",
    "\n",
    "If we take E and F from before, then what is $E + F$or $E\\times F$ or even $E^F$?\n",
    "\n",
    "### ??? (related to states)\n",
    "Let W equal some weight matrix\n",
    "$$\n",
    "W = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\end{bmatrix} \n",
    "$$\n",
    "then $W\\cdot\\mathcal{F}$\n",
    "$$\n",
    "\\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\end{bmatrix}  \\left[ \\begin{array}{c} A \\\\ B \\\\ C \\end{array} \\right] \\implies  \\left[ \\begin{array}{c} B\\rightarrow A \\\\ C\\rightarrow B \\\\ A \\rightarrow C\\end{array} \\right]\\\\\n",
    "$$\n",
    "\n",
    "Ahh, this is where we could do recurrence!\n",
    "But now this is no longer functional. The entries in the matrix have a state and a function.\n",
    "Generating a finite state automata with the matrix, but instead of variables/states, we have ??\n",
    "\n",
    "### Implementation\n",
    "\n",
    "* Just making computation graph, or should it actually evaluate as well?\n",
    "* C(x) + D(z) can only be evaluated properly if;\n",
    "    * the inputs are the right shape, f(x) takes 5x1 but recieves 10x3...\n",
    "    * the outputs are the right shape. f(x) returns 2x1 and g(y) returns 3x5. how can these be composed together?\n",
    "        * We could make all nets input/output the same shape?\n",
    "        * We could create a basis type that all our nets can recieve. Anything else gets (un)raveled?\n",
    "* So normal matnul would be a special case of this? Where A(x) = A * x= ?\n",
    "* How is this related to functional programming? \n",
    "* Based on numpy, should it be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues and fundamental questions;\n",
    "\n",
    "* What would be the basic building block?\n",
    "    * the linear neuron? probably not\n",
    "    * Also these need to be able to proparage forward in time for RNNs.\n",
    "\n",
    "  \n",
    "### Interestingly we could build;\n",
    "* traditional activation functions from a network of less complex neurons. Thus a universal net?\n",
    "* a RNN with neurons that are RNNs themselves, thus giving time-dependent activity, or memory.\n",
    "* So pretty much any neural network architecture is a special case of this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
